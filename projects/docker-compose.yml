version: '3.8'

services:
  # MQTT message broker
  mqtt-broker:
    build: ./mqtt-broker
    container_name: mqtt-broker
    ports:
      - "1883:1883"  # MQTT port
    volumes:
      - mqtt-data:/mosquitto/data
    networks:
      - ai-pipeline
    restart: unless-stopped

  # RTSP server (MediaMTX)
  rtsp-server:
    image: bluenviron/mediamtx:latest
    container_name: rtsp-server
    ports:
      - "8554:8554"  # RTSP port
      - "1935:1935"  # RTMP port
    environment:
      - MTX_PROTOCOLS=tcp
    restart: unless-stopped
    networks:
      - ai-pipeline

  # MP4 streaming media source
  mp4-rtsp-source:
    build: ./mp4-rtsp-source-raw
    container_name: mp4_rtsp_container
    depends_on:
      - rtsp-server
    command: >
      sh -c "echo 'ðŸŽ¬ Starting video stream from $$VIDEO_FILE to $$RTSP_URL...' &&
             if [ ! -f /source-videos/media/$$VIDEO_FILE ]; then
               echo 'âŒ File not found: /source-videos/media/$$VIDEO_FILE' &&
               echo 'ðŸ“ Available files:' && ls -la /source-videos/media/ 2>/dev/null || echo 'Media directory not found' &&
               exit 1;
             fi &&
             ffmpeg -re -stream_loop -1 -i /source-videos/media/$$VIDEO_FILE
                    -c:v copy -preset veryfast -tune zerolatency
                    -c:a aac -f rtsp $$RTSP_URL"
    environment:
      - RTSP_URL=rtsp://rtsp-server:8554/rawstream
      - VIDEO_FILE=rte_far_seg_1_x2.mp4
    volumes:
      - ./media:/source-videos/media:ro  # Mount local media directory
    networks:
      - ai-pipeline
    restart: unless-stopped

  mp4-transcode-sei:
    build: ./mp4-transcoder-sei
    container_name: mp4-transcoder-sei_container
    depends_on:
      - mp4-rtsp-source
    command: 
      - sh
      - -c
      - |
        echo 'ðŸ”„ Starting transcoding from $$IN_RTSP_URL to $$OUT_RTSP_URL...'
        echo 'Testing ffmpeg availability...'
        /usr/local/bin/ffmpeg -version | head -1
        echo 'Testing x264 availability...'
        /usr/local/bin/x264 --version | head -1
        echo 'Starting transcoding pipeline...'
        sleep 2
        /usr/local/bin/ffmpeg -rtsp_transport tcp -i "$$IN_RTSP_URL" -an -f yuv4mpegpipe -pix_fmt yuv420p - | /usr/local/bin/x264 --demuxer y4m --fps 25 --preset fast --tune zerolatency --output - - | /usr/local/bin/ffmpeg -re -f h264 -i - -c:v copy -f rtsp -rtsp_transport tcp "$$OUT_RTSP_URL"
    environment:
      - IN_RTSP_URL=rtsp://rtsp-server:8554/rawstream
      - OUT_RTSP_URL=rtsp://rtsp-server:8554/mystream
    networks:
      - ai-pipeline
    restart: unless-stopped

  # YOLOX object detection service
  yolox-service:
    build:
      context: ./prj-yolox-onnx
      dockerfile: Dockerfile.yaml
    container_name: yolox-service
    depends_on:
      - mqtt-broker
      - rtsp-server
      - mp4-transcode-sei
    volumes:
      - ./pose_estimation_config.yaml:/app/pose_estimation_config.yaml:ro
      - ../contanos:/app/contanos:ro  # Mount local contanos
      - ./prj-yolox-onnx/yolox_main_yaml.py:/app/yolox_main_yaml.py:ro
      - ./prj-yolox-onnx/yolox_worker.py:/app/yolox_worker.py:ro
    command: ["bash", "-lc", "conda activate onnx && python yolox_main_yaml.py --config /app/pose_estimation_config.yaml"]
    networks:
      - ai-pipeline
    restart: unless-stopped
    runtime: nvidia  # Requires GPU support
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - PYTHONPATH=/app

  # RTMPose pose estimation service
  rtmpose-service:
    build:
      context: ./prj-rtmpose-onnx
      dockerfile: Dockerfile.yaml
    container_name: rtmpose-service
    depends_on:
      - mqtt-broker
      - yolox-service
    volumes:
      - ./pose_estimation_config.yaml:/app/pose_estimation_config.yaml:ro
      - ../contanos:/app/contanos:ro  # Mount local contanos
      - ./prj-rtmpose-onnx/rtmpose_main_yaml.py:/app/rtmpose_main_yaml.py:ro
      - ./prj-rtmpose-onnx/rtmpose_worker.py:/app/rtmpose_worker.py:ro
    command: ["bash", "-lc", "conda activate onnx && python rtmpose_main_yaml.py --config /app/pose_estimation_config.yaml"]
    networks:
      - ai-pipeline
    restart: unless-stopped
    runtime: nvidia  # Requires GPU support
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - PYTHONPATH=/app

  # ByteTrack object tracking service
  bytetrack-service:
    build:
      context: ./prj-bytetrack-cpu
      dockerfile: Dockerfile.yaml
    container_name: bytetrack-service
    depends_on:
      - mqtt-broker
      - yolox-service
    volumes:
      - ./pose_estimation_config.yaml:/app/pose_estimation_config.yaml:ro
      - ../contanos:/app/contanos:ro  # Mount local contanos
      - ./prj-bytetrack-cpu/bytetrack_main_yaml.py:/app/bytetrack_main_yaml.py:ro
      - ./prj-bytetrack-cpu/bytetrack_worker.py:/app/bytetrack_worker.py:ro
    command: ["python", "bytetrack_main_yaml.py", "--config", "/app/pose_estimation_config.yaml", ]
    networks:
      - ai-pipeline
    restart: unless-stopped
    environment:
      - PYTHONPATH=/app

  # Annotation visualization service
  annotation-service:
    build:
      context: ./prj-annotation-cpu
      dockerfile: Dockerfile.yaml
    container_name: annotation-service
    depends_on:
      - mqtt-broker
      - rtsp-server
      - bytetrack-service
      - rtmpose-service
    volumes:
      - ./pose_estimation_config.yaml:/app/pose_estimation_config.yaml:ro
      - ../contanos:/app/contanos:ro  # Mount local contanos
      - ./prj-annotation-cpu/annotation_main_yaml.py:/app/annotation_main_yaml.py:ro
      - ./prj-annotation-cpu/annotation_worker.py:/app/annotation_worker.py:ro
    command: ["python", "annotation_main_yaml.py", "--config", "/app/pose_estimation_config.yaml", ]
    networks:
      - ai-pipeline
    restart: unless-stopped
    environment:
      - PYTHONPATH=/app

volumes:
  mqtt-data:

networks:
  ai-pipeline:
    driver: bridge 